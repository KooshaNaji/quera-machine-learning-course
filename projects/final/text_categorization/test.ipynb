{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['description', 'h1', 'h2', 'domain']\n",
    "\n",
    "for col in cols:\n",
    "    most_frequent = train[col].value_counts().sort_values(ascending=False).index[0]\n",
    "    train[col].fillna(most_frequent, inplace=True)\n",
    "    test[col].fillna(most_frequent, inplace=True)\n",
    "    print('All the missing values in column', col, 'are replaced with', most_frequent)\n",
    "train.isna().sum()\n",
    "\n",
    "from hazm import WordTokenizer\n",
    "from hazm import stopwords_list\n",
    "\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    result = tokenizer.tokenize(text)\n",
    "    return result # To-Do\n",
    "\n",
    "def preprocessing(text):\n",
    "    filtered = []\n",
    "    # To-Do\n",
    "    global lst\n",
    "    lst = text.split(' ')\n",
    "    for m in lst:\n",
    "        if m not in stopwords_list():\n",
    "            filtered.append(m)\n",
    "    return filtered\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['category'] = le.fit_transform(train['category'])\n",
    "to_label = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "to_class = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "print(to_label)\n",
    "train.head()\n",
    "# To-Do\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "X_train = train.loc[:3999, 'title'].values\n",
    "y_train = train.loc[:3999, 'category'].values\n",
    "X_val = train.loc[4000:, 'title'].values\n",
    "y_val = train.loc[4000:, 'category'].values\n",
    "X_test = test.loc[:, 'title'].values\n",
    "\n",
    "train_vectors = cv.fit_transform(X_train)\n",
    "val_vectors = cv.transform(X_val)\n",
    "test_vectors = cv.transform(X_test)\n",
    "\n",
    "print(train_vectors.shape, val_vectors.shape)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(train_vectors, y_train)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "predicted = clf.predict(val_vectors)\n",
    "f1_score(y_val,predicted, average='weighted')\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['category'] = clf.predict(test_vectors)\n",
    "submission.replace({0: 'اجتماعی', 1: 'اشتغال', 2: 'تجارت و اقتصاد', 3: 'تحصیلات', 4: 'تکنولوژی و کامپبوتر', 5: 'حقوق و دولت و سیاست', 6: 'حیوانات خانگی', 7: 'خانه و باغبانی', 8: 'خانواده', 9: 'خودرو', 10: 'سفر و گردشگری', 11: 'سلامت', 12: 'علم و دانش', 13: 'غذا و نوشیدنی', 14: 'فیلم و سینما', 15: 'مد و زیبایی', 16: 'مذهبی', 17: 'مسکن', 18: 'موسیقی', 19: 'هنر و سرگرمی', 20: 'ورزش', 21: 'کتاب و ادبیات'}, inplace=True)\n",
    "submission\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
